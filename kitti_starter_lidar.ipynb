{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fS68TJdzxpEc"
      },
      "source": [
        "# **KITTI Stereo Vision - LiDAR depth**\n",
        "\n",
        "Welcome to part 2 of the KITTI Stereo Vision Starter tutorial. In this section we will compare computed stereo depth to LiDAR point clouds, and of course we'll make some cool videos to showcase what we've learned!\n",
        "\n",
        "\n",
        "Once again, the KITTI dataset is located [here]()http://www.cvlibs.net/datasets/kitti/raw_data.php a readme for the KITTI data can be found [here](https://github.com/yanii/kitti-pcl/blob/master/KITTI_README.TXT). A paper that details the data collection can be found [here](http://www.cvlibs.net/publications/Geiger2013IJRR.pdf)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d36mXnbPjREO"
      },
      "source": [
        "### **Download the Data**\n",
        "Now that we have covered a bit of background let's get started! First we will download some data from the KITTI dataset and access it in the collab environment. I am using the raw city data (2011_09_26_drive_0106_sync).\n",
        "\n",
        "This data is synced and rectified. In a future tutorial we will discuss why this is important, for now we will explore the data at a high level."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-oQQrrKRy0N7",
        "outputId": "d4e7380e-e11a-4030-8a2b-028cbb28ad08"
      },
      "outputs": [],
      "source": [
        "!wget https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_26_drive_0106/2011_09_26_drive_0106_sync.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cNRrXKyW1FT2"
      },
      "source": [
        "And now we will download the corresponding calibration file which contains a 3x4 projection matrix."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oMB2UrvJ1EoF",
        "outputId": "bdd8b888-11ae-4baf-f2e9-1bbe433f92ed"
      },
      "outputs": [],
      "source": [
        "!wget https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_26_calib.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AOZ8HpGby1f_"
      },
      "outputs": [],
      "source": [
        "!jar xf 2011_09_26_drive_0106_sync.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u1d45LXK1VV5"
      },
      "outputs": [],
      "source": [
        "!jar xf 2011_09_26_calib.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iQUXTRBq0tpA"
      },
      "source": [
        "In our data folder we have images from 2 gray stereo cameras and 2 color stereo cameras. We also have LiDAR point clouds and GPS and IMU data. The LiDAR used in the KITTI dataset is a Velodyne LiDAR, so the variables that refer to 'velo' actually refer to the LiDAR point cloud.\n",
        "\n",
        "Just as in part1, we will use data from the 2 color cameras. The folder labeled \"image_02\" corresponds to the left camera and \"image_03\" corresponds to the right camera."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NbB7YLLs1790"
      },
      "source": [
        "#### Base Library Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9YX7gGIb1nZb"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from glob import glob\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "plt.rcParams[\"figure.figsize\"] = (20, 10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H5BveFum2HKt"
      },
      "source": [
        "### **Get data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W6-ubgsI2FdU",
        "outputId": "aa39f47c-58db-4dc4-833a-fe798bbbdc95"
      },
      "outputs": [],
      "source": [
        "DATA_PATH = r'2011_09_26/2011_09_26_drive_0106_sync'\n",
        "\n",
        "left_image_paths = sorted(glob(os.path.join(DATA_PATH, 'image_02/data/*.png')))\n",
        "right_image_paths = sorted(glob(os.path.join(DATA_PATH, 'image_03/data/*.png')))\n",
        "\n",
        "# get LiDAR data\n",
        "bin_paths = sorted(glob(os.path.join(DATA_PATH, 'velodyne_points/data/*.bin')))\n",
        "\n",
        "print(f\"Number of left images: {len(left_image_paths)}\")\n",
        "print(f\"Number of right images: {len(right_image_paths)}\")\n",
        "print(f\"Number of LiDAR point clouds: {len(bin_paths)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tunPoQM4aEt8"
      },
      "source": [
        "### **Get Camera Calibration Data**\n",
        "\n",
        "Here we will get calibration data for both color cameras, and we will get the rotation and trnaslation matrix for LiDAR to camera frame of reference.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ua9l9TPaBKF"
      },
      "outputs": [],
      "source": [
        "with open('2011_09_26/calib_cam_to_cam.txt','r') as f:\n",
        "    calib = f.readlines()\n",
        "\n",
        "# get projection matrices\n",
        "P_left = np.array([float(x) for x in calib[25].strip().split(' ')[1:]]).reshape((3,4))\n",
        "P_right = np.array([float(x) for x in calib[33].strip().split(' ')[1:]]).reshape((3,4))\n",
        "\n",
        "# get rectified rotation matrices\n",
        "R_left_rect = np.array([float(x) for x in calib[24].strip().split(' ')[1:]]).reshape((3, 3,))\n",
        "R_right_rect = np.array([float(x) for x in calib[32].strip().split(' ')[1:]]).reshape((3, 3,))\n",
        "\n",
        "R_left_rect = np.insert(R_left_rect, 3, values=[0,0,0], axis=0)\n",
        "R_left_rect = np.insert(R_left_rect, 3, values=[0,0,0,1], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MeQSUNRDdq63"
      },
      "outputs": [],
      "source": [
        "def decompose_projection_matrix(P):    \n",
        "    K, R, T, _, _, _, _ = cv2.decomposeProjectionMatrix(P)\n",
        "    T = T/T[3]\n",
        "\n",
        "    return K, R, T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cjv13fDqdrFG"
      },
      "outputs": [],
      "source": [
        "K_left, R_left, T_left = decompose_projection_matrix(P_left)\n",
        "K_right, R_right, T_right = decompose_projection_matrix(P_right)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QkqRToLrbDhr"
      },
      "source": [
        "Once again: the projection matrices will be decomposed into:\n",
        " - The Intrinsic Calibration Matrix (K) \n",
        " - The rotation Matrix (R)\n",
        " - The translation Matrix (T)\n",
        "\n",
        "The rectified Rotation matrices will be used to help transform a LiDAR point to the camera frame of reference.\n",
        "\n",
        "#### **Get LiDAR to Camera Rotation and Translation Matrices**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YpJ2IVOjaZag",
        "outputId": "de166d10-a75f-4e76-baf6-b983fb5c586d"
      },
      "outputs": [],
      "source": [
        "with open(r'2011_09_26/calib_velo_to_cam.txt', 'r') as f:\n",
        "    calib = f.readlines()\n",
        "\n",
        "R_cam_velo = np.array([float(x) for x in calib[1].strip().split(' ')[1:]]).reshape((3, 3))\n",
        "t_cam_velo = np.array([float(x) for x in calib[2].strip().split(' ')[1:]])[:, None]\n",
        "\n",
        "T_cam_velo = np.vstack((np.hstack((R_cam_velo, t_cam_velo)),\n",
        "                        np.array([0, 0, 0, 1])))\n",
        "\n",
        "T_cam_velo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k6eYGCakbSu5"
      },
      "source": [
        "The LiDAR to Camer translation matrix will allow us to rotate any 3D LiDAR point to the camera pixel space\n",
        "\n",
        "$$ y = P^{(i)}_{rect} R^{(0)}_{rect} T^{cam}_{velo} x,\n",
        "  \\qquad \\text{where } x = [x, y, z, 1]^T $$\n",
        "NOTE:  x is the 3D point in homogeneous coordinates.\n",
        "\n",
        "Since we will be using the left image we will go ahead and make a function to rotate a single LiDAR poin to the left image."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "picPoKc3ZtVq"
      },
      "source": [
        "## **Define functions**\n",
        "We will define all of the necessary functions to compute depth and detect object. Please refer to part 1 for more info on these functions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2S-t880bZ8qU"
      },
      "outputs": [],
      "source": [
        "def compute_sgbm_disparity(left_image, right_image, num_disparities=5*16, \n",
        "                           block_size=11, window_size=5, display=False):\n",
        "    \"\"\" Computes the disparity of an image pair using the SGBM algoithm.\n",
        "        Inputs: \n",
        "            image_left/_right - (MxN) grayscale input images\n",
        "            see opencv documentation for \"StereoBM_create\"\n",
        "        Outputs:\n",
        "            disparity (MxN) computed disparity map for the input images\n",
        "        \n",
        "        NOTE: image_left must be the left image (same for the right) or \n",
        "              unexpected results will occur due to \n",
        "    \"\"\"\n",
        "    # P1 and P2 control disparity smoothness (recommended values below)\n",
        "    P1 = 8 * 3 * window_size**2\n",
        "    P2 = 32 * 3 * window_size**2\n",
        "    sgbm_obj = cv2.StereoSGBM_create(0, num_disparities, block_size, \n",
        "        P1, P2, mode=cv2.STEREO_SGBM_MODE_SGBM_3WAY)\n",
        "        \n",
        "    # compute disparity\n",
        "    disparity = sgbm_obj.compute(left_image, right_image).astype(np.float32)/16.0\n",
        "\n",
        "    # display is desired\n",
        "    if display:\n",
        "      plt.figure(figsize = (40,20))\n",
        "      plt.imshow(disparity, cmap='cividis')\n",
        "      plt.title('Disparity Map', size=25)\n",
        "      plt.show();\n",
        "\n",
        "    return disparity\n",
        "\n",
        "def calc_depth_map(disp_left, K_left, T_left, T_right):\n",
        "    ''' Computes Depth map from Intrinsic Camera Matrix and Translations vectors.\n",
        "        For KITTI, the depth is in meters.\n",
        "        '''\n",
        "    # Get the focal length from the K matrix\n",
        "    f = K_left[0, 0]\n",
        "    \n",
        "    # Get the distance between the cameras from the t matrices (baseline)\n",
        "    b = np.abs(T_left[0] - T_right[0])[0]\n",
        "    \n",
        "    # Replace all instances of 0 and -1 disparity with a small minimum value (to avoid div by 0 or negatives)\n",
        "    disp_left[disp_left <= 0] = 1e-5\n",
        "    \n",
        "    # Calculate the depths \n",
        "    depth_map = f*b / disp_left \n",
        "\n",
        "    return depth_map"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PyJgxGG64J5n"
      },
      "source": [
        "## **Get Object Detection Pipeline**\n",
        "\n",
        "Just like part 1, we will use yolov5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XaKke7tqb48x",
        "outputId": "e4de3464-fe2f-4644-fd69-497acaee59c0"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/ultralytics/yolov5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V4VKQfMzb_pY"
      },
      "outputs": [],
      "source": [
        "!pip install -r yolov5/requirements.txt  #Install whatever is needed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243,
          "referenced_widgets": [
            "940bf9c70c7b42b8882ba8bd50ab0bfd",
            "7df78f266bfc4085b8560c8ea2e08a37",
            "9af1c42ed35a4c57a4618c59fb58531f",
            "6e2314dcd6dd4308820a0c5ba2432c4d",
            "adf12654a30b42208d702454d3e7a0da",
            "80196c77c30248719a7a36b2f5786991",
            "4ebc7a212d8f4c228bf81ea1241e6714",
            "076c647a5f424f36bc3d41b0212a562e",
            "ab939260e04a4f209eb4596d0daaa9a8",
            "547e768c75cd491ea02291961d8830f2",
            "4a539761af224169b7918f1c1552e33d"
          ]
        },
        "id": "qeOx4W3rcBzf",
        "outputId": "12fe5f03-a5ac-4a68-a70a-a632165cf877"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "model = torch.hub.load('ultralytics/yolov5', 'yolov5s')  # or yolov5m, yolov5l, yolov5x, custom"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h_AMAEA_cDee"
      },
      "outputs": [],
      "source": [
        "# set confidence and IOU thresholds\n",
        "model.conf = 0.25  # confidence threshold (0-1), default: 0.25\n",
        "model.iou = 0.25  # NMS IoU threshold (0-1), default: 0.45"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M4_1RjBKcFcr"
      },
      "outputs": [],
      "source": [
        "def get_distances(image, depth_map, bboxes, method='median', draw=True):\n",
        "    ''' Obtains distance measurements for each detected object in the image \n",
        "        Inputs:\n",
        "          image - input image for detection \n",
        "          bboxes - xyxy bounding boxes form detections from yolov5 model output\n",
        "          method - distance metric (median or center)\n",
        "        Outputs:\n",
        "          image - input image with distances drawn at the center of each \n",
        "                  bounding box\n",
        "        '''\n",
        "    for bbox in bboxes:\n",
        "        pt1 = torch.round(bbox[0:2]).to(torch.int).numpy()\n",
        "        pt2 = torch.round(bbox[2:4]).to(torch.int).numpy()\n",
        "        \n",
        "        # get center location on image\n",
        "        x_center = np.round((pt1[1] + pt2[1]) / 2).astype(int)\n",
        "        y_center = np.round((pt1[0] + pt2[0]) / 2).astype(int)\n",
        "\n",
        "        # get depth slice\n",
        "        depth_slice = depth_map[pt1[1]:pt2[1], pt1[0]:pt2[0]]\n",
        "\n",
        "        # compute median depth to get the distance\n",
        "        if method == 'center':\n",
        "            x_c = np.round((pt2[1] - pt1[1]) / 2).astype(int)\n",
        "            y_c = np.round((pt2[0] - pt1[0]) / 2).astype(int)\n",
        "            stereo_depth = depth_slice[x_c, y_c]\n",
        "        else:\n",
        "            stereo_depth = np.median(depth_slice)\n",
        "\n",
        "        # draw depth on image at center of each bounding box\n",
        "        if draw:\n",
        "            cv2.putText(image, \n",
        "                        '{0:.2f} m'.format(stereo_depth), \n",
        "                        (y_center, x_center),\n",
        "                        cv2.FONT_HERSHEY_SIMPLEX, \n",
        "                        0.5, \n",
        "                        (255, 0, 0), 2, cv2.LINE_AA)    \n",
        "        \n",
        "    return image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gotD2zlUcKCx"
      },
      "outputs": [],
      "source": [
        "def get_depth_detections(left_image, right_image, method='median', \n",
        "                         draw_boxes=True, draw_depth=True):\n",
        "    ''' Obtains detections depth estimates for all objects in the input stereo \n",
        "        pair. The images are assumed to have already been rectified. Parameters \n",
        "        in 'compute_sgbm_disparity' will need to be tuned.\n",
        "        '''\n",
        "    # convert to grayscale\n",
        "    left_image_gray = cv2.cvtColor(left_image, cv2.COLOR_RGB2GRAY)\n",
        "    right_image_gray = cv2.cvtColor(right_image, cv2.COLOR_RGB2GRAY)\n",
        "\n",
        "    ## 1. compute left disparity map\n",
        "    disparity = compute_sgbm_disparity(left_image_gray, \n",
        "                                       right_image_gray, \n",
        "                                       num_disparities=96, \n",
        "                                       block_size=7, \n",
        "                                       window_size=7)\n",
        "\n",
        "    ## 2. compute left depth map\n",
        "    depth_map = calc_depth_map(disparity, K_left, T_left, T_right)\n",
        "\n",
        "    # filter depth map\n",
        "    filtered_depth_map = cv2.medianBlur(depth_map, 5)\n",
        "\n",
        "    ## 3. compute detections in the left image\n",
        "    detections = model(left_image)\n",
        "\n",
        "    # draw boxes on image\n",
        "    if draw_boxes:\n",
        "        detections.show() \n",
        "\n",
        "    # get bounding box locations (x1,y1), (x2,y2) Prob, class\n",
        "    bboxes = detections.xyxy[0]\n",
        "\n",
        "    # get distance measurements for image\n",
        "    left_image = get_distances(left_image, \n",
        "                               filtered_depth_map, \n",
        "                               bboxes, \n",
        "                               method, \n",
        "                               draw_depth)\n",
        "\n",
        "    return left_image, filtered_depth_map, bboxes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lX8tyeafcTXJ"
      },
      "source": [
        "We now have our full pipeline from part 1. Now let's find out how to check the accuracy of our stereo depth computations!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1oSB8lk4diF1"
      },
      "outputs": [],
      "source": [
        "index = 100\n",
        "\n",
        "left_image = cv2.cvtColor(cv2.imread(left_image_paths[index]), cv2.COLOR_BGR2RGB)\n",
        "right_image = cv2.cvtColor(cv2.imread(right_image_paths[index]), cv2.COLOR_BGR2RGB)\n",
        "\n",
        "left_image, depth_map, _ = get_depth_detections(left_image, right_image, method='median', \n",
        "                                                draw_boxes=True, draw_depth=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 595
        },
        "id": "FpGLt1tgd3KF",
        "outputId": "dd9dd122-d324-4019-f97c-7ee861ed7979"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "plt.rcParams[\"figure.figsize\"] = (20, 10)\n",
        "\n",
        "_, ax = plt.subplots(2, 1, figsize=(15, 10))\n",
        "\n",
        "ax[0].imshow(left_image);\n",
        "ax[1].imshow(np.log(depth_map), cmap='gist_ncar_r');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7rJ5jWIoeAjG"
      },
      "source": [
        "### **Create LiDAR points pipeline**\n",
        "\n",
        "We will read in the LiDAR point clouds and transform them to the image plane. The code is partially based on the this [repository](https://github.com/azureology/kitti-velo2cam). Once again the information for the calibration parameters is located [here](http://www.cvlibs.net/publications/Geiger2013IJRR.pdf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vuMALLw3bX4U"
      },
      "outputs": [],
      "source": [
        "def get_velo2cam(lidar_bin):\n",
        "    ''' Converts the LiDAR point cloud to camera (u, v, z) image coordinates, \n",
        "        where z is in meters\n",
        "        '''\n",
        "    # read in LiDAR data\n",
        "    scan_data = np.fromfile(lidar_bin, dtype=np.float32).reshape((-1,4))\n",
        "\n",
        "    # convert to homogeneous coordinate system\n",
        "    velo_points = scan_data[:, 0:3] # (x, y, z) --> (front, left, up)\n",
        "    velo_points = np.insert(velo_points, 3, 1, axis=1).T # homogeneous LiDAR points\n",
        "\n",
        "    # delete negative liDAR points\n",
        "    velo_points = np.delete(velo_points, np.where(velo_points[3, :] < 0), axis=1) \n",
        "\n",
        "    # possibly use RANSAC to remove the ground plane for better viewing?\n",
        "\n",
        "    # convert to camera coordinates\n",
        "    velo_camera = P_left @ R_left_rect @ T_cam_velo @ velo_points\n",
        "\n",
        "    # delete negative camera points ??\n",
        "    velo_camera  = np.delete(velo_camera , np.where(velo_camera [2,:] < 0)[0], axis=1) \n",
        "\n",
        "    # get camera coordinates u,v,z\n",
        "    velo_camera[:2] /= velo_camera[2, :]\n",
        "\n",
        "    return velo_camera"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hBItTfOMfQiV"
      },
      "outputs": [],
      "source": [
        "def project_velo2cam(lidar_bin, image):\n",
        "    ''' Projects LiDAR point cloud onto the image coordinate frame '''\n",
        "\n",
        "    # get camera (u, v, z) coordinates\n",
        "    velo_camera = get_velo2cam(lidar_bin)\n",
        "\n",
        "    (u, v, z) = velo_camera\n",
        "\n",
        "    # remove outliers (points outside of the image frame)\n",
        "    img_h, img_w, _ = image.shape\n",
        "    u_out = np.logical_or(u < 0, u > img_w)\n",
        "    v_out = np.logical_or(v < 0, v > img_h)\n",
        "    outlier = np.logical_or(u_out, v_out)\n",
        "    velo_camera = np.delete(velo_camera, np.where(outlier), axis=1)\n",
        "    \n",
        "    return velo_camera"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TH0qobNMhEJP"
      },
      "source": [
        "### **Test the LiDAR projection pipeline**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hraICucUf8vx"
      },
      "outputs": [],
      "source": [
        "index = 220\n",
        "\n",
        "left_image = cv2.cvtColor(cv2.imread(left_image_paths[index]), cv2.COLOR_BGR2RGB)\n",
        "lidar_bin = bin_paths[index]\n",
        "\n",
        "(u, v, z) = project_velo2cam(lidar_bin, left_image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 378
        },
        "id": "dO4qrJ2dhdEU",
        "outputId": "5c6fcf33-9a35-49f7-f2a0-fb804dfa143a"
      },
      "outputs": [],
      "source": [
        "# plot points over iamge\n",
        "plt.imshow(left_image)\n",
        "plt.scatter([u], [v], c=[z], cmap='rainbow_r', alpha=0.5, s=2);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bsisvCGgi3cG"
      },
      "source": [
        "## **Let's go ahead and turn this into a video!**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iy4ZTEJqi20T"
      },
      "outputs": [],
      "source": [
        "from matplotlib import cm\n",
        "\n",
        "rainbow_r = cm.get_cmap('rainbow_r', lut=100)\n",
        "get_color = lambda z : [255*val for val in rainbow_r(int(z.round()))[:3]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZUckVfOzkUKu"
      },
      "outputs": [],
      "source": [
        "result_video = []\n",
        "\n",
        "for idx in range(len(left_image_paths)):\n",
        "\n",
        "    left_image = cv2.cvtColor(cv2.imread(left_image_paths[idx]), cv2.COLOR_BGR2RGB)\n",
        "    lidar_bin = bin_paths[idx]\n",
        "\n",
        "    (u, v, z) = project_velo2cam(lidar_bin, left_image)\n",
        "\n",
        "    # draw LiDAR point cloud on image\n",
        "    for i in range(len(u)):\n",
        "        cv2.circle(left_image, (int(u[i]), int(v[i])), 1, \n",
        "                  get_color(z[i]), -1);\n",
        "    \n",
        "    result_video.append(left_image)\n",
        "\n",
        "\n",
        "# get width and height for video frames\n",
        "h, w, _ = left_image.shape\n",
        "\n",
        "out = cv2.VideoWriter('lidar_proj_2011_09_26.avi',cv2.VideoWriter_fourcc(*'DIVX'), 15, (w,h))\n",
        " \n",
        "for i in range(len(result_video)):\n",
        "    out.write(cv2.cvtColor(result_video[i], cv2.COLOR_BGR2RGB))\n",
        "out.release()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2NuEeEKriTT2"
      },
      "source": [
        "#### Now we need a way to determine the LiDAR measured distance at a certain image coordinate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kBsupwLmlACs"
      },
      "outputs": [],
      "source": [
        "def get_stereo_depth_errors(bboxes, velo_camera, method='median'):\n",
        "    ''' Obtains errors for stereo depth estimation, uses velodyne LiDAR data for\n",
        "        truth reference\n",
        "        '''\n",
        "\n",
        "    errors = []\n",
        "    centers = []\n",
        "\n",
        "    # unpack LiDAR camera coordinates\n",
        "    u, v, z = velo_camera\n",
        "\n",
        "    for bbox in bboxes:\n",
        "        pt1 = torch.round(bbox[0:2]).to(torch.int).numpy()\n",
        "        pt2 = torch.round(bbox[2:4]).to(torch.int).numpy()\n",
        "        \n",
        "        # get center location\n",
        "        x_center = (pt1[1] + pt2[1]) / 2\n",
        "        y_center = (pt1[0] + pt2[0]) / 2\n",
        "\n",
        "        # now get the closest LiDAR points to the center\n",
        "        center_delta = np.abs(np.array((v, u)) \n",
        "                      - np.array([[x_center, y_center]]).T)\n",
        "        \n",
        "        # choose coordinate pair with the smallest L2 norm\n",
        "        min_loc = np.argmin(np.linalg.norm(center_delta, axis=0))\n",
        "\n",
        "        velo_depth = z[min_loc]\n",
        "\n",
        "        # get correpsonding LiDAR Centers\n",
        "        velo_center = np.array([v[min_loc], u[min_loc]])\n",
        "\n",
        "        # get depth slice\n",
        "        depth_slice = depth_map[pt1[1]:pt2[1], pt1[0]:pt2[0]]\n",
        "\n",
        "        # compute median depth to get the distance\n",
        "        if method == 'center':\n",
        "            x_c = np.round((pt2[1] - pt1[1]) / 2).astype(int)\n",
        "            y_c = np.round((pt2[0] - pt1[0]) / 2).astype(int)\n",
        "            stereo_depth = depth_slice[x_c, y_c]\n",
        "        else:\n",
        "            stereo_depth = np.median(depth_slice)\n",
        "\n",
        "        # compute depth error\n",
        "        depth_error = stereo_depth - velo_depth\n",
        "\n",
        "        # add to output array\n",
        "        errors.append([stereo_depth, velo_depth, depth_error])\n",
        "        centers.append([np.array([x_center, y_center]), velo_center])\n",
        "\n",
        "    return np.array(errors), np.array(centers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "snLJbJbOmqSo"
      },
      "outputs": [],
      "source": [
        "index = 220\n",
        "left_image = cv2.cvtColor(cv2.imread(left_image_paths[index]), cv2.COLOR_BGR2RGB)\n",
        "\n",
        "velo_camera = project_velo2cam(lidar_bin, left_image)\n",
        "detections = model(left_image)\n",
        "bboxes = detections.xyxy[0]\n",
        "\n",
        "errors, centers = get_stereo_depth_errors(bboxes, velo_camera, method='median')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3hf7Vb84KKJJ"
      },
      "source": [
        "## **Now make a full pipeline**\n",
        "This pipeline will input a stereo pair along with a LiDAR point cloud. It will compute stereo depth and log error info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ddct0DoEKGYg"
      },
      "outputs": [],
      "source": [
        "def stereo_depth_pipeline(left_image, right_image, lidar_bin, \n",
        "                          draw_boxes=True, method='median'):\n",
        "    ''' Full pipeline to compute stereo depth errors relative to velodyne LiDAR \n",
        "        point cloud data.\n",
        "        ''' \n",
        "\n",
        "    # get depth detections\n",
        "    left_image, depth_map, bboxes = get_depth_detections(left_image, \n",
        "                                                         right_image, \n",
        "                                                         method='median',\n",
        "                                                         draw_boxes=draw_boxes)\n",
        "\n",
        "    # transform and project LiDAR points to camera points\n",
        "    velo_camera = project_velo2cam(lidar_bin, left_image)\n",
        "\n",
        "    # get stereo depth errors\n",
        "    errors, centers = get_stereo_depth_errors(bboxes, \n",
        "                                              velo_camera, \n",
        "                                              method='median')\n",
        "    \n",
        "    return left_image, velo_camera, errors, centers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d4xbrL3WMlcz"
      },
      "outputs": [],
      "source": [
        "index = 220\n",
        "\n",
        "left_image = cv2.cvtColor(cv2.imread(left_image_paths[index]), cv2.COLOR_BGR2RGB)\n",
        "right_image = cv2.cvtColor(cv2.imread(right_image_paths[index]), cv2.COLOR_BGR2RGB)\n",
        "lidar_bin = bin_paths[index]\n",
        "\n",
        "\n",
        "left_image, velo_camera, errors, centers = stereo_depth_pipeline(\n",
        "                                                            left_image, \n",
        "                                                            right_image, \n",
        "                                                            lidar_bin, \n",
        "                                                            draw_boxes=True, \n",
        "                                                            method='median')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SkVzskouPiHL",
        "outputId": "9ce4248f-ee92-4ee5-bd46-6bc6d18d5f1c"
      },
      "outputs": [],
      "source": [
        "errors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 409
        },
        "id": "MnWXnK8BMmEF",
        "outputId": "a0cd37f5-9f3d-41ff-abbe-c7f8fa2ec7b6"
      },
      "outputs": [],
      "source": [
        "u, v, z = velo_camera\n",
        "\n",
        "plt.imshow(left_image)\n",
        "plt.scatter(u, v, c=z, cmap='rainbow_r', alpha=0.5, s=2);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HFBXQcbvt6hc"
      },
      "source": [
        "In this case aside from the clock which is horribly wrong due to it being outside the disparity map. The predicted stereo depth is closer than the true LiDAR depth."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LB71JFVkOykL"
      },
      "source": [
        "## **Now let's make some output videos**\n",
        "\n",
        "While we make these output videos we will also gather error data!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yu43fsGPNYDr"
      },
      "outputs": [],
      "source": [
        "error_metrics = []\n",
        "# result_video = []\n",
        "\n",
        "for idx in range(len(left_image_paths)):\n",
        "    left_image = cv2.cvtColor(cv2.imread(left_image_paths[idx]), cv2.COLOR_BGR2RGB)\n",
        "    right_image = cv2.cvtColor(cv2.imread(right_image_paths[idx]), cv2.COLOR_BGR2RGB)\n",
        "    lidar_bin = bin_paths[idx]\n",
        "\n",
        "    left_image, velo_camera, errors, centers = stereo_depth_pipeline(\n",
        "                                                                left_image, \n",
        "                                                                right_image, \n",
        "                                                                lidar_bin, \n",
        "                                                                draw_boxes=True, \n",
        "                                                                method='median')\n",
        "    \n",
        "    # result_video.append(left_image)\n",
        "\n",
        "    # ensure that errors were recorded\n",
        "    if len(errors) > 0:\n",
        "        error_metrics.append(errors)\n",
        "\n",
        "# get width and height for video frames\n",
        "# h, w, _ = left_image.shape\n",
        "\n",
        "# out = cv2.VideoWriter('boxed_video_2011_09_26.avi',cv2.VideoWriter_fourcc(*'DIVX'), 15, (w,h))\n",
        " \n",
        "# for i in range(len(result_video)):\n",
        "#     out.write(cv2.cvtColor(result_video[i], cv2.COLOR_BGR2RGB))\n",
        "# out.release()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "65qUdEh6SxQm"
      },
      "outputs": [],
      "source": [
        "error_metrics = np.vstack(error_metrics)\n",
        "\n",
        "velo_depth =  error_metrics[:, 1]\n",
        "abs_depth_error = np.abs(error_metrics[:, 2])\n",
        "\n",
        "# remove outliers\n",
        "velo_depth = velo_depth[abs_depth_error < 100]\n",
        "abs_depth_error = abs_depth_error[abs_depth_error < 100]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 599
        },
        "id": "XP8UM8BxSULs",
        "outputId": "eeb7bc22-f0a5-4403-d6cb-45d763adb2a7"
      },
      "outputs": [],
      "source": [
        "plt.scatter(velo_depth, abs_depth_error)\n",
        "plt.title('Velo Depth VS Depth Error');\n",
        "plt.xlabel('Velo Depth (meters)');\n",
        "plt.ylabel('Depth Error (meters');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "akF5wVD2TN1f"
      },
      "source": [
        "We can see from the depth error plot that most detections over 40 meters have a relatively higher stereo depth error than detection of less than 40 meters. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NBDIUA1Yzyh6"
      },
      "source": [
        "## **Now let's run the depth check on another sample of KITTI data!**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jRlNAOcpz2hT",
        "outputId": "28ab5eba-4119-4fc9-9052-ac77ea870518"
      },
      "outputs": [],
      "source": [
        "!wget https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_10_03_drive_0047/2011_10_03_drive_0047_sync.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NLHqD0wgz7_7",
        "outputId": "4f5093dc-5553-489f-f478-f690451535e5"
      },
      "outputs": [],
      "source": [
        "!wget https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_10_03_calib.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KOrNSfVez9V4"
      },
      "outputs": [],
      "source": [
        "!jar xf 2011_10_03_drive_0047_sync.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RMaEguTiz-nf"
      },
      "outputs": [],
      "source": [
        "!jar xf 2011_10_03_calib.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YCzmOHsO0A01"
      },
      "source": [
        "### Get the Calibration data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WpvdfASrz_uo"
      },
      "outputs": [],
      "source": [
        "with open('2011_10_03/calib_cam_to_cam.txt','r') as f:\n",
        "    calib = f.readlines()\n",
        "\n",
        "# get projection matrices\n",
        "P_left = np.array([float(x) for x in calib[25].strip().split(' ')[1:]]).reshape((3,4))\n",
        "P_right = np.array([float(x) for x in calib[33].strip().split(' ')[1:]]).reshape((3,4))\n",
        "\n",
        "# get rectified rotation matrices\n",
        "R_left_rect = np.array([float(x) for x in calib[24].strip().split(' ')[1:]]).reshape((3, 3,))\n",
        "R_right_rect = np.array([float(x) for x in calib[32].strip().split(' ')[1:]]).reshape((3, 3,))\n",
        "\n",
        "R_left_rect = np.insert(R_left_rect, 3, values=[0,0,0], axis=0)\n",
        "R_left_rect = np.insert(R_left_rect, 3, values=[0,0,0,1], axis=1)\n",
        "\n",
        "# decompose projection matrices\n",
        "K_left, R_left, T_left = decompose_projection_matrix(P_left)\n",
        "K_right, R_right, T_right = decompose_projection_matrix(P_right)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nFNQLSlB0Oh8"
      },
      "outputs": [],
      "source": [
        "with open(r'2011_10_03/calib_velo_to_cam.txt', 'r') as f:\n",
        "    calib = f.readlines()\n",
        "\n",
        "R_cam_velo = np.array([float(x) for x in calib[1].strip().split(' ')[1:]]).reshape((3, 3))\n",
        "t_cam_velo = np.array([float(x) for x in calib[2].strip().split(' ')[1:]])[:, None]\n",
        "\n",
        "T_cam_velo = np.vstack((np.hstack((R_cam_velo, t_cam_velo)),\n",
        "                        np.array([0, 0, 0, 1])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HyjFkP_h0U1z",
        "outputId": "bb994e5d-585d-45cd-b858-2e2668bfd7af"
      },
      "outputs": [],
      "source": [
        "DATA_PATH = r'2011_10_03/2011_10_03_drive_0047_sync'\n",
        "\n",
        "left_image_paths = sorted(glob(os.path.join(DATA_PATH, 'image_02/data/*.png')))\n",
        "right_image_paths = sorted(glob(os.path.join(DATA_PATH, 'image_03/data/*.png')))\n",
        "\n",
        "# get LiDAR data\n",
        "bin_paths = sorted(glob(os.path.join(DATA_PATH, 'velodyne_points/data/*.bin')))\n",
        "\n",
        "print(f\"Number of left images: {len(left_image_paths)}\")\n",
        "print(f\"Number of right images: {len(right_image_paths)}\")\n",
        "print(f\"Number of LiDAR point clouds: {len(bin_paths)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MNjYtGZ80kiv"
      },
      "source": [
        "## **Now let's make a video**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MgC2F-eG0aTu"
      },
      "outputs": [],
      "source": [
        "error_metrics = []\n",
        "result_video = []\n",
        "\n",
        "for idx in range(len(left_image_paths)):\n",
        "    left_image = cv2.cvtColor(cv2.imread(left_image_paths[idx]), cv2.COLOR_BGR2RGB)\n",
        "    right_image = cv2.cvtColor(cv2.imread(right_image_paths[idx]), cv2.COLOR_BGR2RGB)\n",
        "    lidar_bin = bin_paths[idx]\n",
        "\n",
        "    left_image, velo_camera, errors, centers = stereo_depth_pipeline(\n",
        "                                                                left_image, \n",
        "                                                                right_image, \n",
        "                                                                lidar_bin, \n",
        "                                                                draw_boxes=True, \n",
        "                                                                method='median')\n",
        "    \n",
        "    # draw LiDAR point cloud on new blank image\n",
        "    u, v, z = velo_camera\n",
        "    new_image = np.zeros_like(left_image, dtype=np.uint8)\n",
        "    for i in range(len(u)):\n",
        "        cv2.circle(new_image, (int(u[i]), int(v[i])), 1, \n",
        "                  get_color(z[i]), -1);\n",
        "    \n",
        "    # stack frames\n",
        "    stacked = np.vstack((left_image, new_image))\n",
        "\n",
        "    # add to result video\n",
        "    result_video.append(stacked)\n",
        "\n",
        "    # ensure that errors were recorded\n",
        "    if len(errors) > 0:\n",
        "        error_metrics.append(errors)\n",
        "\n",
        "# get width and height for video frames\n",
        "h, w, _ = stacked.shape\n",
        "\n",
        "out = cv2.VideoWriter('boxed_pointcloud_stack_2011_10_03.avi',cv2.VideoWriter_fourcc(*'DIVX'), 15, (w,h))\n",
        "\n",
        "# or use mp4\n",
        "# out = cv2.VideoWriter('boxed_pointcloud_stack_2011_10_03.mp4',cv2.VideoWriter_fourcc(*'MP4V'), 15, (w,h))\n",
        " \n",
        "for i in range(len(result_video)):\n",
        "    out.write(cv2.cvtColor(result_video[i], cv2.COLOR_BGR2RGB))\n",
        "out.release()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 595
        },
        "id": "9e-XDksz1Wbg",
        "outputId": "b8af1312-4374-4e40-9cc8-69d31c63d138"
      },
      "outputs": [],
      "source": [
        "plt.imshow(stacked);"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.8.10 ('Python-ddiOOh4g')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "00f03ad223d9bbefba2d85a96e4c14bf4b7cfec3ac1501740897c115facd9986"
      }
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "076c647a5f424f36bc3d41b0212a562e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4a539761af224169b7918f1c1552e33d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4ebc7a212d8f4c228bf81ea1241e6714": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "547e768c75cd491ea02291961d8830f2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6e2314dcd6dd4308820a0c5ba2432c4d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_547e768c75cd491ea02291961d8830f2",
            "placeholder": "",
            "style": "IPY_MODEL_4a539761af224169b7918f1c1552e33d",
            "value": " 14.1M/14.1M [00:00&lt;00:00, 19.0MB/s]"
          }
        },
        "7df78f266bfc4085b8560c8ea2e08a37": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_80196c77c30248719a7a36b2f5786991",
            "placeholder": "",
            "style": "IPY_MODEL_4ebc7a212d8f4c228bf81ea1241e6714",
            "value": "100%"
          }
        },
        "80196c77c30248719a7a36b2f5786991": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "940bf9c70c7b42b8882ba8bd50ab0bfd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7df78f266bfc4085b8560c8ea2e08a37",
              "IPY_MODEL_9af1c42ed35a4c57a4618c59fb58531f",
              "IPY_MODEL_6e2314dcd6dd4308820a0c5ba2432c4d"
            ],
            "layout": "IPY_MODEL_adf12654a30b42208d702454d3e7a0da"
          }
        },
        "9af1c42ed35a4c57a4618c59fb58531f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_076c647a5f424f36bc3d41b0212a562e",
            "max": 14808437,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ab939260e04a4f209eb4596d0daaa9a8",
            "value": 14808437
          }
        },
        "ab939260e04a4f209eb4596d0daaa9a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "adf12654a30b42208d702454d3e7a0da": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
